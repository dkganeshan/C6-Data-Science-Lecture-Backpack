{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) are a type of artificial intelligence model designed to understand and generate human-like text. They are often built using deep learning techniques, particularly using architectures like transformers. GPT models, developed by OpenAI, are among the most well-known examples of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, we'll need to install the needed packages, like TensorFlow, PyTorch or Hugging Face's Transformers library, which will provide easy access to pre-trained language models.\n",
    "\n",
    "pip install torch tranformers\n",
    "\n",
    "pip install accelerate -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load up a pre-trained model using our libraries. We can load up the GPT-2 model as follows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the chosen library to load the pre-trained model and its corresponding tokenizer. The tokenizer is used to convert text into numerical inputs that the model can understand, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is loaded, we can now use it to generate text. We provide a prompt and the model will predict what the continuation of the text will be.\n",
    "\n",
    "You can specify parameters like maximum length, number of sequences to generate, and whether to sample from the model or use greedy decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time my father was about to open his mouth to me after saying, \"You're beautiful. Go inside the chapel and see your mom-and-dad!\" I kept telling him, \"Daddy, you know you are beautiful. Look\n",
      "Once upon a time, this city, built for the rich, stood on a pedestal.\n",
      "\n",
      "No new towns were built, but the old grew, the new lost.\n",
      "\n",
      "There was no new land to dig up, no new farmers\n",
      "Once upon a time, though at no cost, we could have been back home, and then again, we wouldn't have been here. After all, the universe itself was not far removed. (p. 28)\n",
      "\n",
      "I'm not asking\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=3, do_sample=True)\n",
    "\n",
    "for sequence in output:\n",
    "    text = tokenizer.decode(sequence, skip_special_tokens=True)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a specific task or dataset, you might want to fine-tune the pre-trained model on your data. This involves further training the model with your dataset to adapt it to your specific needs. Fine-tuning can significantly improve the performance of the model on your task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pewn\\Desktop\\Endgame Shenanigans\\Large Language Models\\.vevn\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Define your dataset and dataloaders\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TextDataset(tokenizer\u001b[38;5;241m=\u001b[39m\u001b[43mtokenizer\u001b[49m, file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, block_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m      5\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForLanguageModeling(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      7\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./gpt2-finetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     overwrite_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     save_total_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     13\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# Define your dataset and dataloaders\n",
    "dataset = TextDataset(tokenizer=tokenizer, file_path=\"data.txt\", block_size=128)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of language models like Large Language Models (LLMs) can be evaluated using various metrics depending on the specific task or application. Here are some common evaluation metrics:\n",
    "\n",
    "Perplexity: Perplexity is a standard metric used to evaluate language models. It measures how well a probability model predicts a sample. Lower perplexity indicates better performance. For LLMs, perplexity is calculated by exponentiating the average negative log-likelihood of the test set.\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy): BLEU is commonly used for evaluating the quality of machine-translated text. It measures the similarity between the machine-generated text and one or more reference translations. BLEU scores range from 0 to 1, with higher scores indicating better performance.\n",
    "\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation): ROUGE is used for evaluating the quality of summaries or paraphrases. It measures the overlap between the generated text and reference summaries in terms of n-grams, word sequences, and word pairs.\n",
    "\n",
    "Human Evaluation: Human evaluation involves having human judges assess the quality of the generated text based on various criteria such as fluency, coherence, relevance, and overall quality. Human evaluation is subjective but provides valuable insights into the performance of language models, especially in real-world applications where human judgment is crucial.\n",
    "\n",
    "Task-Specific Metrics: For tasks like text classification, named entity recognition, sentiment analysis, etc., task-specific metrics such as accuracy, F1 score, precision, recall, and area under the ROC curve (AUC) are used to evaluate the performance of language models.\n",
    "\n",
    "Downstream Task Performance: In many cases, the ultimate evaluation of a language model's performance is its effectiveness in downstream tasks. This involves fine-tuning the pre-trained model on a specific task (e.g., sentiment analysis, question answering) and evaluating its performance on a separate validation set using task-specific metrics.\n",
    "\n",
    "User Satisfaction: In real-world applications like chatbots or virtual assistants, user satisfaction metrics such as user ratings, engagement metrics, and feedback surveys are essential for evaluating the performance of language models in providing a satisfactory user experience.\n",
    "\n",
    "It's important to select appropriate evaluation metrics based on the specific task or application and interpret the results in the context of the intended use case. Additionally, it's common to use a combination of objective metrics and human judgment to comprehensively evaluate the performance of language models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hehe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
