{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity is a commonly used metric to evaluate the performance of language models. It measures how well a probability model predicts a sample of data. In the context of natural language processing (NLP) and language modeling, perplexity is used to assess how well a language model predicts a sequence of words or tokens.\n",
    "\n",
    "Perplexity is calculated based on the probability assigned by the language model to a given sequence of tokens. It is defined as the exponentiation of the cross-entropy loss per token.\n",
    "\n",
    "Lower Perplexity: A lower perplexity indicates that the language model is better at predicting the given sequence of tokens. In other words, the model is more confident and makes more accurate predictions. Lower perplexity values are desirable, and they indicate better language model performance.\n",
    "\n",
    "Higher Perplexity: Conversely, a higher perplexity indicates that the language model is less confident and makes less accurate predictions. The model finds the sequence of tokens more surprising or less probable according to its learned distribution. Higher perplexity values suggest poorer language model performance.\n",
    "\n",
    "Interpretation: Perplexity can be interpreted as the average branching factor of the language model. A perplexity of \n",
    "\n",
    "N suggests that, on average, the language model has \n",
    "\n",
    "N equally likely choices at each step of prediction. Lower perplexity values correspond to models that are more certain about their predictions.\n",
    "\n",
    "Perplexity provides a quantitative measure of how well a language model predicts sequences of tokens. Lower perplexity values indicate better model performance and higher confidence in predictions. It is a crucial metric used in evaluating and comparing different language models, especially in tasks like machine translation, language modeling, and text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 25.136091232299805\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Define a custom dataset class for loading test data\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, text_file, tokenizer, block_size):\n",
    "        self.examples = []\n",
    "        with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "            for i in range(0, len(tokenized_text) - block_size + 1, block_size):\n",
    "                self.examples.append(tokenizer.build_inputs_with_special_tokens(tokenized_text[i:i+block_size]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.examples[idx])\n",
    "\n",
    "# Load pre-trained GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = TestDataset(\"data.txt\", tokenizer, block_size=128)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Evaluate perplexity\n",
    "total_loss = 0.0\n",
    "num_tokens = 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch[:, :-1], batch[:, 1:]\n",
    "        inputs, labels = inputs.to(model.device), labels.to(model.device)\n",
    "        outputs = model(inputs, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), reduction='sum')\n",
    "        total_loss += loss.item()\n",
    "        num_tokens += labels.size(1)\n",
    "\n",
    "perplexity = torch.exp(torch.tensor(total_loss) / torch.tensor(num_tokens))\n",
    "print(\"Perplexity:\", perplexity.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".vevn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
